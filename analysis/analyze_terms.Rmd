---
title: "Analyze Textbook Index Terms"
output: github_document
---

A term that appears in the index of a textbook has been selected by the author as a subject to highlight and make more accessible to the reader. In this sense, a term in the index is "important."

# Setup

Load necessary packages and hard-code data locations. Note that the data-loading chunks below assume you have run the textbook scraper locally (i.e., that `masterlist.csv` is ready to go).

```{r}
library(tidyverse)
library(googlesheets4)
# remotes::install_github("coolbutuseless/ggpattern")
library(ggpattern)
filename <- "../data_proc/masterlist.csv"
url_metadata <- "https://docs.google.com/spreadsheets/d/1n_fFptcgPIzzlNYHkUblP_cReWGNo3TKGJ-Bo7W5oA0/edit#gid=0"
url_manual <- "https://docs.google.com/spreadsheets/d/1TMl_vxOytHYPd9_CAJoJNJAkbWnj9FMUtxLg5R8ufyo/edit#gid=0"
```

Authenticate your Google account for Google Sheets access.

```{r include=FALSE}
gs4_auth(email = "legomannqc@gmail.com")
```

## Load metadata

Load the book metadata from our shared Google Sheet.

```{r}
df_meta_raw <- read_sheet(
    url_metadata, col_types = str_c(c("cccc", rep("?", 19)), collapse = "")
  )
df_meta_raw %>%
  glimpse()
```

Process the metadata.

```{r}
df_meta <-
  df_meta_raw %>%
  select(
    authors = Author,
    title = Title,
    ISBN = `ISBN 13`,
    courselist = Courselist,
    known_courses = `Known Courses`,
    need_ocr = `OCR Needed`,
    include = Inclusion,
    have_pdf = `PDF Stored`,
    no_index = `No Index`,
  ) %>%
  mutate(
    include = include == "Include",
    courselist_institution = str_extract(known_courses, "\\([\\w|\\s]+\\)$") %>%
      str_remove_all(., "[:punct:]"),
  ) %>%
  filter(!is.na(authors))
df_meta
```

## Load scraped PDF data

This comes from the locally-stored `masterlist.csv` file.

```{r}
df_raw <- read_csv(
  filename,
  col_types = "cc"
)
df_raw
```

## Load manual data


```{r}
df_manual_raw <- read_sheet(
  url_manual,
  col_types = str_c(c("cccc", rep("?", 22)), collapse = "")
)
df_manual_raw %>%
  glimpse()
```

Process manual data

```{r}
df_manual <-
  df_manual_raw %>%
  pivot_longer(
    cols = -c(Authors, Title, `ISBN-13`, Assigned),
    names_to = "term",
    values_to = "PRESENT"
  ) %>%
  filter(PRESENT == "Y") %>%
  mutate(
    term = str_remove(term, "\\n.*$")
  ) %>%
  select(
    term,
    ISBN = `ISBN-13`,
  )

df_manual

df_manual %>%
  distinct(ISBN) %>%
  count()
```


## Process term data

Process the Index term data and join with manual data.

```{r}
df_data <-
  df_raw %>%
  rename(term = Term) %>%
  # Atomize to single words
  # separate_rows(term) %>%
  # Lowercase
  mutate(term = str_to_lower(term)) %>%
  bind_rows(df_manual)

df_data
```

## Sanity-check available PDFs

Are any of the fully-digital PDFs unaccounted?

```{r}
df_meta %>%
  filter(include, have_pdf, !need_ocr) %>%
  distinct(ISBN, .keep_all = TRUE) %>%
  anti_join(df_data, by = "ISBN")
```

- No unaccounted fully-digital books!

I've used the chunk above to track down "missing" PDFs; most of these were due to ISBN's that did not match between the PDF filename and the metadata (Google) sheet.

## Need OCR

Which books do we need to run through OCR?

```{r}
df_meta %>%
  filter(include, have_pdf, need_ocr) %>%
  select(authors, title)
```

Do we have all OCR books in our manual list?

```{r}
df_meta %>%
  filter(include, need_ocr) %>%
  anti_join(df_manual_raw, by = c("ISBN" = "ISBN-13"))
```

- We don't have Moran & Shapiro 7th ed, but we do have the 6th ed

Do we have all OCR books coded?

```{r}
df_manual_raw %>%
  filter(if_all(everything(), ~!is.na(.x))) %>% 
  anti_join(
    df_manual_raw,
    .,
    by = "ISBN-13"
  )
```

- Carrier et al.: We have a hardcopy somewhere... working on it
- Moran & Shapiro: Edition mismatch
- Turns & Haworth: ILL placed, still waiting

## Special cases

```{r}
df_data %>%
  semi_join(
    .,
    df_meta %>%
      filter(
        # str_detect(authors, "Kerrebrock"), # Scraper throws weird msg
        ISBN == "9780471947219" # Sheppard & Tongue; OCR'd at library
      ),
    by = "ISBN"
  )
```

## Missing digitized indexes

Are any books missing from the digitized corpus?

```{r}
df_meta %>%
  filter(include, courselist, !need_ocr, !no_index) %>%
  anti_join(df_data, by = "ISBN") %>%
  select(title, everything())
```

- No missing digitized indices

## Missing any books

```{r}
df_meta %>% 
  filter(include, courselist) %>%
  anti_join(df_data, by = "ISBN")
```

- Gonzales & Ferrer: Has no Index section (DNE)

- Carryer et al.: In progress...
- Turns & Haworth: ILL placed...
- Moran & Shapiro: We have the 6th edition

# Analyze

## Write full list

```{r write-full-list}
df_meta %>%
  filter(include, courselist) %>%
  select(
    ISBN, courselist_institution, no_index
  ) %>%
  ## Compute artifact recovery
  left_join(
    df_data %>%
      distinct(ISBN) %>%
      mutate(recovered = TRUE)
  ) %>%
  replace_na(list(recovered = FALSE)) %>%
  arrange(courselist_institution) %>%
  rename(
    `Institution` = courselist_institution,
    `No Index` = no_index,
    Recovered = recovered,
  ) %>%
  write_csv("../data_proc/list_pub.csv")
```


## Describe corpus

### Courselist books

```{r}
df_courselist_counts <-
  df_meta %>%
  filter(include, courselist) %>%
  count(courselist_institution) %>%
  arrange(desc(n)) %>%
  rename(
    Institution = courselist_institution,
    `Reserve Books` = n
  ) %>%
  mutate(
    Institution = str_replace(Institution, "CalPoly", "CalPoly SLO"),
  )

df_courselist_counts %>%
  knitr::kable()

df_courselist_counts %>%
  summarize(n_total = sum(`Reserve Books`))
```

- the number of (double-counted) books listed across the 5 course reserve lists

### Unique corselist books (Index or no)

```{r}
df_meta %>%
  filter(include, courselist) %>%
  distinct(ISBN) %>%
  count()
```

### Unique corselist books with Index

```{r}
df_meta %>%
  filter(include, courselist, !no_index) %>%
  distinct(ISBN) %>%
  count()
```

- the number of unique books listed across the 5 course reserve lists

### Digital Index

```{r}
df_meta %>%
  filter(include, courselist, !no_index, !need_ocr) %>%
  semi_join(df_data, by = "ISBN") %>%
  distinct(ISBN) %>%
  count()
```

- the number of fully-digital Index sections

### Manually-processed Indexes

```{r}
df_meta %>%
  filter(include, courselist, !no_index, need_ocr) %>%
  semi_join(df_data, by = "ISBN") %>%
  distinct(ISBN) %>%
  count()
```

### No Index

```{r}
df_meta %>%
  filter(include, courselist, no_index) %>%
  distinct(ISBN, .keep_all = TRUE)
```

### Unobtained books

```{r}
df_meta %>%
  filter(include, courselist, !no_index) %>%
  anti_join(df_data, by = "ISBN") %>%
  distinct(ISBN, .keep_all = TRUE)
```

- Moran & Shapiro: We have a different edition
- Turns & Haworth: ILL request placed
- Carryer et al.: We have it somewhere....

### Most-occurring books

```{r}
df_meta %>%
  filter(include, courselist) %>%
  count(ISBN) %>%
  arrange(desc(n)) %>%
  left_join(
    .,
    df_meta %>%
      distinct(ISBN, .keep_all = TRUE) %>%
      select(ISBN, title)
  )
```

- at most, a particular textbook was listed 2x

## Statistics-related titles

### Full corpus

```{r}
df_meta %>%
  filter(include, courselist) %>%
  summarize(
    n_total = n(),
    n_stats = sum(str_detect(str_to_lower(title), "statis|data"))
  ) %>%
  mutate(frac_stat = n_stats / n_total)
```


### Missing books

```{r}
df_meta %>%
  filter(include, courselist) %>%
  anti_join(df_data, by = "ISBN") %>%
  summarize(
    n_total = n(),
    n_stats = sum(str_detect(str_to_lower(title), "statis|data"))
  ) %>%
  mutate(frac_stat = n_stats / n_total)
```

### Fisher exact test

```{r}
## Set up contingency table
n_corpus_stats <-
  df_meta %>%
  filter(
    include,
    courselist,
    str_detect(str_to_lower(title), "statis|data")
  ) %>%
  semi_join(df_data, by = "ISBN") %>%
  count() %>%
  pull(n)

n_Ncorpus_stats <-
  df_meta %>%
  filter(
    include,
    courselist,
    str_detect(str_to_lower(title), "statis|data")
  ) %>%
  anti_join(df_data, by = "ISBN") %>%
  count() %>%
  pull(n)

n_corpus_Nstats <-
  df_meta %>%
  filter(
    include,
    courselist,
    !str_detect(str_to_lower(title), "statis|data")
  ) %>%
  semi_join(df_data, by = "ISBN") %>%
  count() %>%
  pull(n)

n_Ncorpus_Nstats <-
  df_meta %>%
  filter(
    include,
    courselist,
    !str_detect(str_to_lower(title), "statis|data")
  ) %>%
  anti_join(df_data, by = "ISBN") %>%
  count() %>%
  pull(n)

mat_counts <-
  matrix(
    c(n_corpus_stats, n_corpus_Nstats, n_Ncorpus_stats, n_Ncorpus_Nstats),
    nrow = 2
)
mat_counts

## Run the test
fisher.test(mat_counts)
```

The fraction of statistics-related titles among the missing books is not significantly different from fraction in the corpus. **NO LONGER RELEVANT** due to obtaining virtually every book....

## Count terms

### Definitions

The following code detects the presence of certain keywords in the textbook indexes. The `count` represents the number of textbooks whose index includes the keyword, while the `frac` represents the fraction (of our available corpus) that includes the keyword.

Note that some of the terms are multiply-defined; for instance, `tradeoff` is counted if either `trade` or `tradeoff` is detected in the Index.

```{r}
# Define search terms through helper functions
term_summaries <- list(
  ## Physics
  "acceleration" = ~max(str_detect(.x, "acceleration")),
  "force" = ~max(str_detect(.x, "force")),
  "load" = ~max(str_detect(.x, "load")),
  "pressure" = ~max(str_detect(.x, "pressure")),
  "strength" = ~max(str_detect(.x, "strength")),
  "stress" = ~max(str_detect(.x, "stress")),
  ## Engineering design
  "cost" = ~max(str_detect(.x, "cost")),
  "design" = ~max(str_detect(.x, "design")),
  "failure" = ~max(str_detect(.x, "failure|fail")),
  "maximize" = ~max(str_detect(.x, "maximize|maximization")),
  "minimize" = ~max(str_detect(.x, "minimize|minimization")),
  "optimize" = ~max(str_detect(.x, "optimize|optimization")),
  "safety" = ~max(str_detect(.x, "safety")),
  "safety factor" = ~max(str_detect(.x, "safety factor|factor of safety")),
  "tradeoff" = ~max(str_detect(.x, "tradeoff|trade")),
  ## Uncertainty
  "error" = ~max(str_detect(.x, "error")),
  "probability" = ~max(str_detect(.x, "probability|probabilities")),
  "statistics" = ~max(str_detect(.x, "statistic")),
  "tolerance" = ~max(str_detect(.x, "tolerance")),
  "uncertainty" = ~max(str_detect(.x, "uncertainty|uncertain")),
  "variability" = ~max(str_detect(.x, "variability")),
  ## Mixed / ambiguous
  "variation" = ~max(str_detect(.x, "variation")),
  "limit" = ~max(str_detect(.x, "limit"))
)

df_classes <- tribble(
  ~term, ~class,
  "acceleration", "physics",
  "force", "physics",
  "load", "physics",
  "pressure", "physics",
  "strength", "physics",
  "stress", "physics",

  "cost", "design",
  "design", "design",
  "failure", "design",
  "maximize", "design",
  "minimize", "design",
  "optimize", "design",
  "safety", "design",
  "safety factor", "design",
  "tradeoff", "design",

  ## Uncertainty
  "probability", "uncertainty",
  "statistics", "uncertainty",
  "tolerance", "uncertainty",
  "uncertainty", "uncertainty",
  "variability", "uncertainty",

  ## Mixed use
  "error", "(mixed)",
  "variation", "(mixed)",
  "limit", "(mixed)",
)
```

### Run count

```{r}
# Run analysis
df_counts <-
  df_data %>%
  semi_join(
    .,
    df_meta %>%
      filter(include, courselist),
    by = "ISBN"
  ) %>%
  group_by(ISBN) %>%
  summarize(across(term, term_summaries)) %>%
  summarize(
    across(-ISBN, sum),
    total = n()
  ) %>%
  pivot_longer(
    cols = -total,
    names_to = "term",
    values_to = "count"
  ) %>%
  mutate(
    term = str_remove_all(term, "term_"),
    frac = round(count / total, digits = 2)
  ) %>%
  arrange(desc(count)) %>%
  left_join(df_classes) %>%
  select(term, class, count, frac)

df_counts %>%
  knitr::kable()
```

```{r}
## Visualize
ar = 1 / 4
df_counts %>%
  # filter(class != "(mixed)") %>%
  mutate(term = fct_reorder(term, -frac)) %>%

  ggplot(aes(term, frac)) +
  geom_col_pattern(
    aes(
      fill = class,
      pattern_shape = class, 
    ), 
    pattern = 'pch',
    pattern_density = 0.6,
    pattern_color = "white",
    color = "black",
  ) +

  scale_y_continuous(
    limits = c(0, 1),
    labels = scales::label_percent(),
  ) +
  scale_fill_discrete(guide = "none") +
  scale_pattern_shape_discrete(name = "Term Class") +
  rzdr::theme_common() +
  theme(
    axis.text.x = element_text(angle = 270, hjust = 0),
    legend.position = "bottom",
    # legend.direction = "vertical",
    aspect.ratio = ar,
  ) +
  labs(
    x = "Index Term",
    y = "Books Including Term",
  )

ggsave(
  "../images/textbooks-terms.png",
  width = 8,
  height = 5,
  bg = "white"
)
```

*Observations*

- Physics-related terms dominate the list; `force` is important in the vast majority of engineering textbooks (`r df_counts %>% filter(term == "force") %>% pull(frac)`).
- Uncertainty-related terms appear less frequently:
  - `probability` is important in `r df_counts %>% filter(term == "probability") %>% pull(frac)` of textbooks. This is similar to `frac` of `strength`
  - `uncertainty` is important in `r df_counts %>% filter(term == "uncertainty") %>% pull(frac)` of textbooks. This is similar to `frac` of `optimize`
  - `tolerance` is important in `r df_counts %>% filter(term == "tolerance") %>% pull(frac)` of textbooks. This is a very small fraction, but `tolerance` is quite a bit more specific than something like `uncertainty`
- "Error" shows up an intermediate number of times, but as EF has shown, the term has a highly-variable meaning to practicing engineers.
  - `error` is important in `r df_counts %>% filter(term == "error") %>% pull(frac)` of textbooks

## Relative occurrence

```{r}
## Extract counts
n_force <- df_counts %>% 
  filter(term == "force") %>% 
  pull(count)

df_counts %>% 
  mutate(r = round(n_force / count, 2)) %>% 
  select(term, class, r) %>% 
  
  filter(term %in% c("uncertainty", "statistics", "tolerance", "safety factor"))
```


## Count categories

How many books contain *any* keyword from one of the categories?

```{r}
# Run analysis
df_count_classes <-
  df_data %>%
  semi_join(
    .,
    df_meta %>%
      filter(include, courselist),
    by = "ISBN"
  ) %>%
  group_by(ISBN) %>%
  summarize(across(term, term_summaries)) %>% 
  pivot_longer(
    cols = -ISBN,
    names_to = "term",
    values_to = "present"
  ) %>% 
  mutate(term = str_remove(term, "term_")) %>% 
  left_join(
    df_classes
  ) %>% 
  group_by(class, ISBN) %>% 
  summarize(present = max(present)) %>% 
  summarize(n = sum(present)) %>% 
  arrange(desc(n))

df_count_classes
```

# Per-institution counts

```{r}
df_counts_inst <- 
  map_dfr(
    c("UCLA", "MIT", "SUNY Poly", "CalPoly", "CMU"),
    function(inst) {
      df_data %>%
      semi_join(
        .,
        df_meta %>%
          filter(include, courselist, courselist_institution == inst),
        by = "ISBN"
      ) %>%
      group_by(ISBN) %>%
      summarize(across(term, term_summaries)) %>%
      summarize(
        across(-ISBN, sum),
        total = n()
      ) %>%
      pivot_longer(
        cols = -total,
        names_to = "term",
        values_to = "count"
      ) %>%
      mutate(
        term = str_remove_all(term, "term_"),
        frac = round(count / total, digits = 2)
      ) %>%
      arrange(desc(count)) %>%
      left_join(df_classes) %>%
      select(term, class, count, frac) %>% 
      mutate(inst = inst)
    }
  )

df_counts_inst
```

Visualize

```{r}
## Visualize
df_counts_inst %>%
  rename(class = class) %>% 
  mutate(
    term = fct_reorder(term, -frac, .fun = sd),
    inst = str_replace(inst, "CalPoly", "CalPoly\nSLO"),
    inst = str_replace(inst, "SUNY Poly", "SUNY\nPoly"),
  ) %>%

  ggplot(aes(term, frac, color = inst)) +
  geom_point(size = 1) +
  geom_line(aes(group = inst), size = 0.2) +

  scale_y_continuous(
    limits = c(0, 1),
    labels = scales::label_percent(),
  ) +
  # scale_fill_discrete(guide = "none") +
  scale_color_discrete(name = "Institution") +
  # scale_pattern_shape_discrete(name = "Term Class") +
  facet_wrap(~class, scales = "free") +
  rzdr::theme_common() +
  theme(
    axis.text.x = element_text(size = 10, angle = 270, hjust = 0),
    axis.text.y = element_text(size = 10),
    strip.text.y = element_text(hjust = 0),
    legend.position = "bottom",
    # legend.direction = "vertical",
    aspect.ratio = 1 / 3,
  ) +
  labs(
    x = "Index Term",
    y = "Books Including Term",
  )

ggsave(
  "../images/textbooks-terms-inst.png",
  width = 6.5,
  # height = 5,
  bg = "white"
)
```

# Visualizing a collection

Pick on CalPoly SLO

```{r}
df_data %>%
  semi_join(
    .,
    df_meta %>%
      filter(include, courselist, courselist_institution == "CalPoly"),
    by = "ISBN"
  ) %>%
  group_by(ISBN) %>%
  summarize(across(term, term_summaries))
```

